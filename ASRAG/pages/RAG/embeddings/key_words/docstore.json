{"docstore/data": {"f0c3b0dc-5531-4658-9108-de5dba626622": {"__data__": {"id_": "f0c3b0dc-5531-4658-9108-de5dba626622", "embedding": null, "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": ""}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f17539b1-7770-464f-865b-61e017f7f8cc", "node_type": "4", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": ""}, "hash": "0136a5b5f9a965ce8e9efa12c4325609a55de202e4643c775dd58ed53128dd32", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7d898e8-024f-48b3-8b43-42852104a39f", "node_type": "1", "metadata": {}, "hash": "32b5ec6868faab5e9e31c010537e69ff945c58695160cc362a1819e65911988a", "class_name": "RelatedNodeInfo"}}, "text": "", "start_char_idx": 0, "end_char_idx": 0, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e7d898e8-024f-48b3-8b43-42852104a39f": {"__data__": {"id_": "e7d898e8-024f-48b3-8b43-42852104a39f", "embedding": null, "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "12e1ac98-936e-42c5-9d74-ed4380e1b47e", "node_type": "4", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance"}, "hash": "1f280d03aa3309b8f14b5db1f788ca59b7d694b6fe87a6b37ff8b0d5b51aec06", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0c3b0dc-5531-4658-9108-de5dba626622", "node_type": "1", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": ""}, "hash": "0136a5b5f9a965ce8e9efa12c4325609a55de202e4643c775dd58ed53128dd32", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "05c80f1b-e6a6-4ab6-acf0-ff0c0823b277", "node_type": "1", "metadata": {}, "hash": "8b1e8cc18c0038ce84debcfb1e03cb710ef288b6212342ec6b8d94cde545635e", "class_name": "RelatedNodeInfo"}}, "text": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance\n\nLingjiao Chen, Matei Zaharia, James Zou\nStanford University", "start_char_idx": 1, "end_char_idx": 151, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05c80f1b-e6a6-4ab6-acf0-ff0c0823b277": {"__data__": {"id_": "05c80f1b-e6a6-4ab6-acf0-ff0c0823b277", "embedding": null, "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "# Abstract"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1b41fc57-e6a8-424d-8b30-55512106d4ac", "node_type": "4", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "# Abstract"}, "hash": "82cc0353633c674e763ec61ea5d0b3d6f95c869ff1e6e01f467c724a27bafbe0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7d898e8-024f-48b3-8b43-42852104a39f", "node_type": "1", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance"}, "hash": "c3f851ebc1e64a06052801fa8a66f6dea925ffdfbd77f8e8ffa8c0c28ad2a7de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1dbb4a56-82a7-451b-a142-4cb73aaeb41e", "node_type": "1", "metadata": {}, "hash": "69f921bd99cb1047a6e98a570534512893b4994d9fcb43efa13defa59cb33574", "class_name": "RelatedNodeInfo"}}, "text": "# Abstract\n\nThere is a rapidly growing number of large language models (LLMs) that users can query for a fee. We review the cost associated with querying popular LLM APIs\u2014e.g. GPT-4, ChatGPT, J1-Jumbo\u2014and find that these models have heterogeneous pricing structures, with fees that can differ by two orders of magnitude. In particular, using LLMs on large collections of queries and text can be expensive. Motivated by this, we outline and discuss three types of strategies that users can exploit to reduce the inference cost associated with using LLMs: 1) prompt adaptation, 2) LLM approximation, and 3) LLM cascade. As an example, we propose FrugalGPT, a simple yet flexible instantiation of LLM cascade which learns which combinations of LLMs to use for different queries in order to reduce cost and improve accuracy. Our experiments show that FrugalGPT can match the performance of the best individual LLM (e.g. GPT-4) with up to 98% cost reduction or improve the accuracy over GPT-4 by 4% with the same cost. The ideas and findings presented here lay a foundation for using LLMs sustainably and efficiently.", "start_char_idx": 0, "end_char_idx": 1112, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1dbb4a56-82a7-451b-a142-4cb73aaeb41e": {"__data__": {"id_": "1dbb4a56-82a7-451b-a142-4cb73aaeb41e", "embedding": null, "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "# Introduction"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c056585-412e-4b72-94f5-579ca13e03ea", "node_type": "4", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "# Introduction"}, "hash": "d318bef82cde5c7ae47447cf1684bda09006064b0da7d1b090df12c93aefcf47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05c80f1b-e6a6-4ab6-acf0-ff0c0823b277", "node_type": "1", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "# Abstract"}, "hash": "0a4247beb51ae14f5fd9fc1add893ba4b08b60f189057f26ba8bc7b4d7f3cef2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "606ade99-9106-496c-92ee-2936ed1edb5b", "node_type": "1", "metadata": {}, "hash": "a463ea6853ff3d9206784518d951f87dc792f8b5662e9614dac38e441f19953f", "class_name": "RelatedNodeInfo"}}, "text": "# Introduction\n\nWe are in the midst of an explosion of large language models (LLMs). The alluring possibilities of using LLMs for large-scale applications such as commerce, science, and finance have led a growing number of companies (OpenAI, AI21, CoHere, etc.) to offer LLMs as services.\n\nWhile LLMs such as GPT-4 achieves unprecedented performance in tasks such as question answering, using them for high-throughput applications can be very expensive. For example, ChatGPT is estimated to cost over $700,000 per day to operate [Cosa], and using GPT-4 to support customer service can cost a small business over $21,000 a month [Cosb]. In addition to the financial cost, using the largest LLMs encures substantial environmental and energy impact [BGMMS21, WRG + 22], affecting the social welfare of current and future generations.\n\nThere are many LLMs now available via APIs and they charge heterogeneous prices. The cost of using a LLM API typically consists of three components: 1) prompt cost (proportional to the length of the prompt), 2) generation cost (proportional to the generation length), and 3) sometimes a fixed cost per query. We compared the cost associated with using 12 different commercial LLMs from mainstream providers including OpenAI, AI21, CoHere and Textsynth (Table 1). Their cost can differ by up to 2 orders of magnitudes: for example, the prompt cost for 10M tokens is $30 for OpenAI\u2019s GPT-4 but only $0.2 for GPT-J hosted by Textsyth.\n\nGiven the heterogeneous cost and quality, how to effectively and efficiently leverage the full set of LLM options is a key challenge for pracitioners. If the tasks are relatively simple, then aggregating multiple responses from GPT-J [WK21] (whose size is 30x smaller than GPT-3) offers performance similar to GPT-3 [ANC + 22], leading to financial and environmental savings. However, the performance of GPT-J can be much worse on difficult tasks [TLI +23]. Moreover, relying on one API provider is not reliable if that provider becomes unavailable, potentially due to spiking demand. Existing model ensemble paradigms such as model cascade [VJ04, WLM11] and FrugalML [CZZ20, CZZ22] were designed for predictive tasks with a known set of labels and do not account for the full capabilities of LLM. How to use LLMs affordably and accurately therefore calls for new approaches.\n\nOur contributions. In this paper, we lay out our vision of a flexible framework that uses LLM APIs to process natural language queries within a budget constraint, termed FrugalGPT. As shown in Figure 1\n---", "start_char_idx": 0, "end_char_idx": 2547, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "606ade99-9106-496c-92ee-2936ed1edb5b": {"__data__": {"id_": "606ade99-9106-496c-92ee-2936ed1edb5b", "embedding": null, "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Query"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "96877b40-1d5f-4930-afc9-55a97ce603ea", "node_type": "4", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Query"}, "hash": "6785f7601c925aeb129d576101cda93519e6c33b5c414046d086e07e7230aad6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1dbb4a56-82a7-451b-a142-4cb73aaeb41e", "node_type": "1", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "# Introduction"}, "hash": "35579c8c55e4ed892980e1354d093e06e37c2dd6b645fb66ae237033beb76654", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c20661c-50e8-4944-8083-d55712ff2edc", "node_type": "1", "metadata": {}, "hash": "f285ba6c83a1ff135cfa68aec5eba21a04cbb778ce3e473c597a26de6928f803", "class_name": "RelatedNodeInfo"}}, "text": "Query\n\n|GPT-4|Answer|\n|---|---|\n|(a) Existing LLM Usage|0.86|\n|Zero-shot Few-shot ... CoT|0.84|\n|GPT-Neo|0.82|\n|ChatGPT|0.8|", "start_char_idx": 1, "end_char_idx": 125, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c20661c-50e8-4944-8083-d55712ff2edc": {"__data__": {"id_": "6c20661c-50e8-4944-8083-d55712ff2edc", "embedding": null, "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Query"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8e16fc9f-534d-405f-80a8-38fd1681ccf7", "node_type": "4", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Query"}, "hash": "f92f92c3aa5cbaad67a1ead39817a4ff27d870c4e9bb15e49412357d06e61cb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "606ade99-9106-496c-92ee-2936ed1edb5b", "node_type": "1", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Query"}, "hash": "4e7106b08f8a6edff464fcd1249e21f01eb7b3851f5369f197253593c48dfc84", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d07e0fb-15d0-4957-9e3b-6df8e0572f68", "node_type": "1", "metadata": {}, "hash": "89723a43e7dbcdda673659e027a21378626d265e82e92f2d8f9ef1e69d078d6b", "class_name": "RelatedNodeInfo"}}, "text": "Query\n\n|LLM|0.78|\n|---|---|\n|LLM Approximation| |\n|Prompt Cascade|Accuracy|\n|Adaptation|0.76|\n|J1-L|0.74|\n|FSQ FQ| |", "start_char_idx": 1, "end_char_idx": 117, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d07e0fb-15d0-4957-9e3b-6df8e0572f68": {"__data__": {"id_": "1d07e0fb-15d0-4957-9e3b-6df8e0572f68", "embedding": null, "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Budget"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c6339632-993f-4bfd-ba19-dadb3e70fd37", "node_type": "4", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Budget"}, "hash": "378f547c6c4e5522935420c13251db38be3b6c69b28e219a16040fe9063895dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c20661c-50e8-4944-8083-d55712ff2edc", "node_type": "1", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Query"}, "hash": "87abd2882c0bc359a52537f87af193b5bd9ab48a189100186006a50ce45a30af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "79b89db4-c2c3-4c81-a0e1-4f8c02c5a2ee", "node_type": "1", "metadata": {}, "hash": "03704b534cb310791189a34def8111a6b06f7020cfc4b0a7eb08ea5f334ee96c", "class_name": "RelatedNodeInfo"}}, "text": "Budget\n\n|Approximation|0.7|GPT-C|10|20|30|40|\n|---|---|---|---|---|---|---|\n|GPT-J| |ChatGPT| |GPT-4| |Cost ($)|", "start_char_idx": 1, "end_char_idx": 113, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "79b89db4-c2c3-4c81-a0e1-4f8c02c5a2ee": {"__data__": {"id_": "79b89db4-c2c3-4c81-a0e1-4f8c02c5a2ee", "embedding": null, "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Figure 1: Our vision for reducing LLM cost while improving accuracy."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "921c6ce1-b555-414d-bfa7-51b2bb7da68a", "node_type": "4", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Figure 1: Our vision for reducing LLM cost while improving accuracy."}, "hash": "d5f62beba638703d51d463a33b8d06894dded75a4b2287d358068a388e4663e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d07e0fb-15d0-4957-9e3b-6df8e0572f68", "node_type": "1", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Budget"}, "hash": "fe7c58d70b811d8fe3f40aed2a6bfaeee2567e246db87cc12097daac6c02e713", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d79d00a-64d0-44eb-9b3e-054da2854340", "node_type": "1", "metadata": {}, "hash": "025842a073f5a57fae16a13bef4040af5d375e89dbd82abba6738d0d4e19eecc", "class_name": "RelatedNodeInfo"}}, "text": "Figure 1: Our vision for reducing LLM cost while improving accuracy.\n\n(a) The standard usage sends queries to a single LLM (e.g. GPT-4), which can be expensive.\n\n(b) Our proposal is to use prompt adaption, LLM approximation and LLM cascade to reduce the inference cost. By optimizing over the selection of different LLM APIs (e.g., GPT-J, ChatGPT, and GPT-4) as well as prompting strategies (such as zero-shot [BMR + 20], few-shot [LSZ +21], and chain-of-thought(CoT) [WWS +22]), we can achieve substantial efficiency gains.\n\n(c) On HEADLINES (a financial news dataset), FrugalGPT can reduce the inference cost by 98% while exceeding the performance of the best individual LLM (GPT-4).", "start_char_idx": 1, "end_char_idx": 686, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d79d00a-64d0-44eb-9b3e-054da2854340": {"__data__": {"id_": "8d79d00a-64d0-44eb-9b3e-054da2854340", "embedding": null, "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "1, we discuss three main strategies for cost reduction:"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9f3fd947-0e73-417e-bf3a-e0b168e6edb8", "node_type": "4", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "1, we discuss three main strategies for cost reduction:"}, "hash": "59158376161041716510e737cda9efe409c74f666bc8e8a8885a67c33bf70535", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "79b89db4-c2c3-4c81-a0e1-4f8c02c5a2ee", "node_type": "1", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Figure 1: Our vision for reducing LLM cost while improving accuracy."}, "hash": "0638f614fd2d3ad811c7e835b6ac814abc9b9ba228ad9a53210cbccc7dfdc6ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5fd8b287-39ba-48eb-a041-fdcf9494f262", "node_type": "1", "metadata": {}, "hash": "eb2db4c5752dfc77a10967e884062ea33c32ae1bc625d3a6a50cd1ceccff2c9f", "class_name": "RelatedNodeInfo"}}, "text": "1, we discuss three main strategies for cost reduction:\n\nprompt adaptation, LLM approximation, and LLM cascade. The prompt adaptation explores how to identify effective (often shorter) prompts to save cost. LLM approximation aims to create simpler and cheaper LLMs to match a powerful yet expensive LLM on specific tasks. LLM cascade focuses on how to adaptively choose which LLM APIs to use for different queries.\n\nTo illustrate the potential of these ideas, we implement and evaluate a simple version of FrugalGPT using LLM cascade. On each dataset and task, FrugalGPT learns how to adaptively triage different queries in the dataset to different combinations of LLMs, including ChatGPT [Cha], GPT-3 [BMR + 20] and GPT-4 [Ope23]. Our experiments show that FrugalGPT can save up to 98% of the inference cost of the best individual LLM API while matching its performance on the downstream task. On the other hand, FrugalGPT can improve the performance by up to 4% with the same cost. We believe this is only the tip of the iceberg and we hope FrugalGPT opens a new window toward reducing LLMs\u2019 inference cost and improving its performances.", "start_char_idx": 1, "end_char_idx": 1141, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5fd8b287-39ba-48eb-a041-fdcf9494f262": {"__data__": {"id_": "5fd8b287-39ba-48eb-a041-fdcf9494f262", "embedding": null, "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Related Works"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0d9b0dea-c6b0-464e-8759-23a455fa7555", "node_type": "4", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Related Works"}, "hash": "39a9125a0f6d8991af845f81aee75b7c46a8d6944917bf82e6f889ebdceb3b90", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d79d00a-64d0-44eb-9b3e-054da2854340", "node_type": "1", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "1, we discuss three main strategies for cost reduction:"}, "hash": "1ca6867a49ba7ae052a4694e3dddf7abfe4d2edf9f6f6ddf5a77d1fa904689df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b532ead4-cc32-4cdd-8c65-d0d219af4807", "node_type": "1", "metadata": {}, "hash": "4ef15b3c23b176b4a1b634f89a08a855939ef68d46ef51436e985eb9e421099e", "class_name": "RelatedNodeInfo"}}, "text": "Related Works\n\nPrompt Engineering. Prompt engineering has emerged as a discipline for crafting prompts to enhance LLMs\u2019 performance across various applications. Recent developments include few-shot [BMR + 20], chain-of-thought [WWS + 22], knowledge enhancement [LLL + 21, KSL +22], and numerous other prompting techniques [MDL + 23, KTF + 22, ZSH + 22, DGSG22]. Existing prompt engineering approaches often aim to provide more detailed task explanations and in-context examples, resulting in longer and more expensive prompts. In contrast, this paper explores the use of concise prompts to reduce costs.\n\nModel Ensemble. Model ensembles, which involve combining multiple ML models for prediction, have gained popularity in supervised learning [VJ04, Fri02], unsupervised learning [YLLL14], semi-supervised learning [GDMR22], and weakly supervised learning [DSP + 17]. Model ensembles typically require white-box access to multiple models for training purposes, but LLM APIs are often black-box. Moreover, model ensembles necessitate querying all models for a single query, thereby increasing costs.\n\nSystem Optimization for LLMs. Numerous efforts have aimed to accelerate the training and inference time of modern deep learning models through system optimization [HMD15, CHSV17, Cas19, JZA19, RRWN11]. Recent work focuses on post-training quantization [BHS + 22, YLW + 23, XLS +22], training pipeline parallelism [LZG + 21], and hardware-aware pruning [KFA23] tailored for LLMs. System optimization requires modifications to LLMs\u2019 internal states (e.g., model weights), but many commercial\n---", "start_char_idx": 1, "end_char_idx": 1594, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b532ead4-cc32-4cdd-8c65-d0d219af4807": {"__data__": {"id_": "b532ead4-cc32-4cdd-8c65-d0d219af4807", "embedding": null, "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "LLM APIs and ML-as-a-Service"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "008d72aa-528f-487c-8b90-59d2c0e05907", "node_type": "4", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "LLM APIs and ML-as-a-Service"}, "hash": "56e98e00cb4433c1c5546b1ee74356c3409b8a72aea6324c73d663c1e9be2a8b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5fd8b287-39ba-48eb-a041-fdcf9494f262", "node_type": "1", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Related Works"}, "hash": "8d76db5da3976419a466af942474ace3173e7f55cf413336a7bc7336bb173549", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26abc728-9d71-42b8-93ec-1697a7f125a8", "node_type": "1", "metadata": {}, "hash": "28c99e65de70a4cffe32144734612b1dcdb41280c9449653b7d6cc5f4f3e2c6b", "class_name": "RelatedNodeInfo"}}, "text": "LLM APIs and ML-as-a-Service\n\nLLM APIs do not release their models. Additionally, the rapidly increasing size of LLMs renders retraining highly expensive. LLM APIs constitute a crucial component of the rapidly expanding machine-learning-as-a-service (MLaaS) industry. Recent studies have demonstrated the diversity of different ML APIs\u2019 predictions [BG18, KNL + 20, CCZZ21] and proposed strategies for leveraging various classification ML APIs to improve performance [CZZ20, CZZ22]. The outputs of LLM APIs encompass the entire natural language space, but existing work requires a fixed (and known) label set. Moreover, both prompt choices and LLM API selections significantly impact generative tasks\u2019 performance, resulting in a considerably larger optimization space than standard classification.", "start_char_idx": 1, "end_char_idx": 799, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "26abc728-9d71-42b8-93ec-1697a7f125a8": {"__data__": {"id_": "26abc728-9d71-42b8-93ec-1697a7f125a8", "embedding": null, "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Organization of the Paper"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a677f3e2-6f11-4bc1-9fb7-57f347a154ed", "node_type": "4", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Organization of the Paper"}, "hash": "a75b0cd5ca4e7b653424a22db82821c6b317bf592ffb7d21bb65a98e1e102de7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b532ead4-cc32-4cdd-8c65-d0d219af4807", "node_type": "1", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "LLM APIs and ML-as-a-Service"}, "hash": "7f319c59c703166d27977a5b7e5073ec367c1a9bad70e7428a844a044a7eaf2c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77cea1c8-7602-4149-9c6f-4f0759104eb8", "node_type": "1", "metadata": {}, "hash": "f452802d2a64f70f8b770507ff3eff9b91497b8e2cf8dbc01d28cab69b80a5b9", "class_name": "RelatedNodeInfo"}}, "text": "Organization of the Paper\n\nThe remaining part of the paper is organized as follows. We start by offering more context and the problem statement in Section 2. Next in Section 3, we present our visions on how to use LLM APIs affordably and accurately. Section 4 shows the empirical benefits of FrugalGPT using real-world LLM APIs (including GPT-3, ChatGPT, and GPT-4). Finally, we discuss future prospects in Section 5.", "start_char_idx": 1, "end_char_idx": 418, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "77cea1c8-7602-4149-9c6f-4f0759104eb8": {"__data__": {"id_": "77cea1c8-7602-4149-9c6f-4f0759104eb8", "embedding": null, "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Scope and Problem Statement"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "66a13b36-b55f-4903-9662-51f875698037", "node_type": "4", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Scope and Problem Statement"}, "hash": "6648d29f124cb6d6d6bdd93c866bbb56bffd07be4e760f0706ebced96c12983d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "26abc728-9d71-42b8-93ec-1697a7f125a8", "node_type": "1", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Organization of the Paper"}, "hash": "13531de92aa949f1904a4aeaf114d3b3b6a978105aeb87718aa82a258e0f888b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a9c809a-2f7e-4c52-a139-3b4267b0a822", "node_type": "1", "metadata": {}, "hash": "973ee082fbeaa6756281143a00b1809f4fbb733b110c435b58e5e21fc422f1f6", "class_name": "RelatedNodeInfo"}}, "text": "Scope and Problem Statement\n\nNatural language query answering. In this paper, we concentrate on the standard natural language query answering task, where the objective is to answer a query q sampled from a natural language query distribution Q. Various real-world natural language tasks, such as news classification, reading comprehension, and commonsense reasoning, can be formulated as query-answering problems.\n\nLLM marketplace. We consider answering queries via the LLM market, which comprises K different LLM APIs, denoted by {fi(\u00b7)}K. Each fi(\u00b7): P \u2192 A is a function that, given a prompt p from the prompt space P, generates an answer from the answer distribution A. Note that to use LLM APIs, one has to convert each query q to some corresponding prompt first. LLM APIs are associated with their own cost, typically consisting of three components: a portion proportional to the length of the prompt, a portion proportional to the length of the generated answer, and (sometimes) a fixed cost per query. Formally, given a prompt p, the cost of using the ith LLM API is denoted by c(p), where c(p) = \u02dcci\u2016fi(p)\u2016 + \u02dcci\u2016p\u2016 + \u02dcci, where \u02dcci, i = 0, 1, 2 are constants.\n\nAn illustrative example. Adapting the case study provided by [Cosa], assume a small business operates a customer service using GPT-4. The company caters to 15,000 customers each month, with each customer asking three questions twice a week, totaling 360,000 queries per month. Suppose for each question, its corresponding prompt averages 1800 tokens, and the answer is around 80 tokens. Considering that the input and response costs of GPT-4 are $0.03 and $0.06 per thousand tokens, respectively, the total monthly cost amounts to 360 \u00d7 ($0.03 \u00d7 1800 + $0.06 \u00d7 80) \u2248 $21.2K. Such a high cost is prohibitive for many small businesses.\n\nProblem statement: budget-aware LLM API usage. Our primary goal in this paper is leveraging LLM APIs within a budget constraint. Formally, this can be formulated as maximizing the overall task performance E[(q,a)\u2208Q\u00d7A r(a, \u02c6a(s, q))], while ensuring the average cost is bounded by a user-defined value b, i.e., E[c(s, q)] \u2264 b. Here, a denotes the correct answer to the query q, \u02c6a(s, q) is the generated answer by some strategy s for query q, and c(s, q) is the associated cost for processing query q using strategy s. The reward function r(\u00b7, \u00b7) measures how closely the generated answer aligns with the correct one. It is crucial to note that the search space for the strategy is vast, encompassing factors such as which prompts to use, which LLM APIs to employ, and how to aggregate their responses.", "start_char_idx": 1, "end_char_idx": 2607, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8a9c809a-2f7e-4c52-a139-3b4267b0a822": {"__data__": {"id_": "8a9c809a-2f7e-4c52-a139-3b4267b0a822", "embedding": null, "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "How to Use LLMs Affordably and Accurately"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7868ef7a-460f-45bf-a886-f1eb69966961", "node_type": "4", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "How to Use LLMs Affordably and Accurately"}, "hash": "0f28321e2935476d976b588cfe00dbb1ef84e6b32988e1c55db8cb5143e02c31", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77cea1c8-7602-4149-9c6f-4f0759104eb8", "node_type": "1", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Scope and Problem Statement"}, "hash": "1c5c1aedab561b722f81f615cf82cfe56b7cf34afa428dcc57346b5434ddd207", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71b1f06f-defc-4a8f-9c76-37a8b1788f3a", "node_type": "1", "metadata": {}, "hash": "49d8159da49c381cfb0dedc7b95ca1019cfc563f93bcb066b151d3c11e5ed4c9", "class_name": "RelatedNodeInfo"}}, "text": "How to Use LLMs Affordably and Accurately\n\nNow we present our vision on how to use LLM APIs within a budget. As shown in Figure 1 (b), we discuss three cost-reduction strategies, namely, prompt adaptation, LLM approximation, and LLM cascade.\n---\n|Content|Page Number|\n|---|---|\n|Figure 2: Illustrations of cost-saving strategies.|4|\n|(a) Prompt selection uses a subset of in-context examples as the prompt to reduce the size of the prompt.| |\n|(b) Query concatenation aggregates multiple queries to share prompts.| |\n|(c) Completion cache stores and reuses an LLM API\u2019s response when a similar query is asked.| |\n|(d) Model fine-tuning uses expensive LLMs\u2019 responses to fine-tune cheap LLMs.| |\n|(e) LLM cascade employs different LLM APIs for different queries.| |\n---", "start_char_idx": 1, "end_char_idx": 769, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71b1f06f-defc-4a8f-9c76-37a8b1788f3a": {"__data__": {"id_": "71b1f06f-defc-4a8f-9c76-37a8b1788f3a", "embedding": null, "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Strategy 1: Prompt adaptation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4f5d3b28-49ec-4969-aa29-a97dbb2322d9", "node_type": "4", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Strategy 1: Prompt adaptation"}, "hash": "2ecb7da64a6b510c3db819345a4c42cf8cc6967c1795ca5248bb4883512f60e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a9c809a-2f7e-4c52-a139-3b4267b0a822", "node_type": "1", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "How to Use LLMs Affordably and Accurately"}, "hash": "46fc4246d0b9fc44f93e63d214019379e7f1440832c1c7448d1b625aa5325ffd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47ba160e-b309-4e00-b846-4ad36333b462", "node_type": "1", "metadata": {}, "hash": "9d2709f901cebb84ae93b22529549aa48fd59d56631e4f35c1584ee371c5fe96", "class_name": "RelatedNodeInfo"}}, "text": "Strategy 1: Prompt adaptation\n\nThe cost of an LLM query increases linearly with the size of the prompt. Consequently, a logical approach to reduce the cost of using LLM APIs involves decreasing the prompt\u2019s size, a process we refer to as prompt adaptation. Prompt selection (as illustrated in Figure 2 (a)) is a natural example of prompt adaptation: rather than employing a prompt containing numerous examples that demonstrate how to perform a task, one can retain a small subset of examples in the prompt. This results in a smaller prompt and subsequently lower cost. An intriguing challenge of prompt selection lies in determining which examples to maintain for various queries without compromising task performance.\n\nAn additional instantiation is query concatenation (Figure 2 (b)). It is important to note that processing queries individually necessitates sending the same prompt to an LLM API multiple times. Therefore, the fundamental concept of query concatenation involves sending the prompt only once to the LLM API while allowing it to address multiple queries, thereby preventing redundant prompt processing. To accomplish this, several queries must be concatenated into a single query, and the prompt must explicitly request the LLM API to process multiple queries. For instance, to handle two queries using one prompt, the examples presented in the prompt can include both queries followed by their corresponding answers.", "start_char_idx": 1, "end_char_idx": 1436, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47ba160e-b309-4e00-b846-4ad36333b462": {"__data__": {"id_": "47ba160e-b309-4e00-b846-4ad36333b462", "embedding": null, "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Strategy 2: LLM approximation"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "95428b11-3349-4ffc-ae2a-f65c60bad8dc", "node_type": "4", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Strategy 2: LLM approximation"}, "hash": "6b6fc46a6159f71d83b6c659a64c5e5205f7dfa7ab7fb874b51b6db59972990e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71b1f06f-defc-4a8f-9c76-37a8b1788f3a", "node_type": "1", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Strategy 1: Prompt adaptation"}, "hash": "e2b23d394c6c7672fbde9042a259c4e28e102ef877bae1256cd1258bae541bd2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "997d78a4-9eda-4ef5-98be-1de70a453fde", "node_type": "1", "metadata": {}, "hash": "fbfa71f6edd982164c246dd82c60fd2d408a29b601e80307eb4772b3a13033c4", "class_name": "RelatedNodeInfo"}}, "text": "Strategy 2: LLM approximation\n\nThe concept of LLM approximation is quite simple: if an LLM API is too costly to utilize, one can approximate it using more affordable models or infrastructures. One example is the completion cache: as depicted in Figure 2 (c), the fundamental idea involves storing the response locally in a cache (e.g., a database) when submitting a query to an LLM API. To process a new query, we first verify if a similar query has been previously answered. If so, the response is retrieved from the cache. An LLM API is invoked only if no similar query is discovered in the cache. The completion cache provides substantial cost savings when similar queries are frequently posed. For instance, consider a search engine powered by an LLM API. If numerous users search for the same or similar keywords simultaneously, the completion cache facilitates answering all their queries by invoking the LLM only once.\n\nAnother example of LLM approximation is model fine-tuning. As shown in Figure 2(d), this process consists of three steps: first, collect a powerful but expensive LLM API\u2019s responses to a few queries; second, use the responses to fine-tune a smaller and more affordable AI model; and finally, employ the fine-tuned model for new queries. In addition to cost savings, the fine-tuned model often does not require lengthy prompts, thus providing latency improvements as a byproduct.", "start_char_idx": 1, "end_char_idx": 1406, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "997d78a4-9eda-4ef5-98be-1de70a453fde": {"__data__": {"id_": "997d78a4-9eda-4ef5-98be-1de70a453fde", "embedding": null, "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Strategy 3: LLM cascade"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2c537797-d5cf-418c-be60-8634f22d589c", "node_type": "4", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Strategy 3: LLM cascade"}, "hash": "087040e91b6b31028dac94149cacad752df3dca304f14403ae472f5fb3220af8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47ba160e-b309-4e00-b846-4ad36333b462", "node_type": "1", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Strategy 2: LLM approximation"}, "hash": "47a0fdc1a383e17cbe667cd15525736117462b57e6d2d1813b9ad7eef594b84d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c79362ad-30d9-42f6-82d8-a815d0f069ef", "node_type": "1", "metadata": {}, "hash": "4cf3de7111c2ebd1cf32b5e589dbef982af2ba4b43aac8802ed81406a52084f2", "class_name": "RelatedNodeInfo"}}, "text": "Strategy 3: LLM cascade\n\nThe increasing availability of LLM APIs with heterogeneous performance and costs presents a unique opportunity for data-adaptive LLM selection. Different LLM APIs have their own strengths and weaknesses for various queries. Consequently, appropriately selecting which LLMs to use can provide both cost reduction and performance improvements. LLM cascade, as illustrated in Figure 2 (e), is one such example. LLM cascade sends a query to a list of LLM APIs sequentially. If one LLM API\u2019s response is reliable, then its response is returned, and no further LLMs in the list are needed. The remaining LLM APIs are queried only if the previous APIs\u2019 generations are deemed insufficiently reliable. Query cost is significantly reduced if the first few APIs are relatively inexpensive and produce reliable generations.\n\nThe key components of LLM cascade consist of two elements: (i) a generation scoring function and (ii) an LLM router. The generation scoring function, denoted by g(\u00b7,\u00b7) : Q \u00d7 A 7 \u2192[0, 1], generates a reliability score given a query and an answer produced by an LLM API. The LLM router selects m LLM APIs to include in the list. Let Lm \u2208 [K] denote the indexes of the m APIs selected by the router. Given a new query, it iteratively invokes the ith API in the list to obtain an answer fLi(q). Then, it uses the scoring function to generate a score g(q, fLi(q)). It returns the generation if the score is higher than a threshold \u03c4i, and queries the next service otherwise.\n\nThe scoring function can be obtained by training a simple regression model that learns whether a generation is correct from the query and a generated answer. Learning the selected list LL and the\n---\nthreshold vectors \u03c4\n\n\u03c4\n\n\u03c4 can be modeled as a constraint optimization problem:\n\n|max|E[r(a, f Lz (q))]|\n|---|---|\n|L|[ \u2211z ]|\n|s.t.|Ei=1 c Li,2\u2016f L i(q)\u2016 + \u02dcc Li,1\u2016q\u2016 + \u02dccL i,0 \u2264 b,|\n|z = arg ming(q, f (q)) \u2265 \u03c4| |\n|i Li \u03c4 i| |\n\nHere, z denotes the LLM API at which the router stops and returns the answer, the first constraint ensures the average cost is bounded by the budget, and the objective measures the quality of the generation fLz (q) for a query q compared to the true answer a. This problem is inherently a mixed-integer optimization and thus computationally expensive to solve. To address this issue, we develop a specialized optimizer that (i) prunes the search space of LL by ignoring any list of LLMs with small answer disagreement, and (ii) approximates the objective by interpolating it within a few samples. This results in an efficient implementation with satisfactory performance, as shown later in Figure 5.\n\nCompositions. Combining approaches within and across different strategies can lead to further cost reduction and performance enhancement. For instance, joint prompt and LLM selection is a composition of prompt selection and LLM cascade: for a given query, it searches for the smallest prompt and most affordable LLM that achieves satisfactory task performance. Another example is to search across both existing LLM APIs and fine-tuned models. It is important to note that the composition of different approaches also increases the computational costs for training. Consequently, this paves the way for investigating trade-offs between query costs, task performance, and computational costs.\n\nLLM Cascade Reduces Cost and Improves Accuracy\n\nIn this section, we present an empirical study on the FrugalGPT LLM cascade. Our goals are three-fold: (i) understand what a simple instantiation of LLM cascade learns, (ii) quantify the cost savings attained by FrugalGPT while matching the best individual LLM API\u2019s performance, and (iii) measure the trade-offs between performance and cost enabled by FrugalGPT.\n\nSetups: LLM APIs, Tasks, Datasets, and FrugalGPT instances. We have selected 12 LLM APIs from 5 mainstream providers, namely, OpenAI [Ope], AI21 [AI2], CoHere [CoH], Textsynth [Tex], and ForeFrontAI [FFA]. The details are summarized in Table 1.", "start_char_idx": 1, "end_char_idx": 3988, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c79362ad-30d9-42f6-82d8-a815d0f069ef": {"__data__": {"id_": "c79362ad-30d9-42f6-82d8-a815d0f069ef", "embedding": null, "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Strategy 3: LLM cascade"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2c537797-d5cf-418c-be60-8634f22d589c", "node_type": "4", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Strategy 3: LLM cascade"}, "hash": "087040e91b6b31028dac94149cacad752df3dca304f14403ae472f5fb3220af8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "997d78a4-9eda-4ef5-98be-1de70a453fde", "node_type": "1", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Strategy 3: LLM cascade"}, "hash": "eb439d41c6e0420ae58d6b7312276873936c34f0e9a08fd117a84acc40b71ce7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62d129b0-50b8-41e2-86ff-56c8e6b669f0", "node_type": "1", "metadata": {}, "hash": "af50c5767c87e81c57522f1ce8df15e1743f792e96906219f49e2ebfb74b27bb", "class_name": "RelatedNodeInfo"}}, "text": "Consequently, this paves the way for investigating trade-offs between query costs, task performance, and computational costs.\n\nLLM Cascade Reduces Cost and Improves Accuracy\n\nIn this section, we present an empirical study on the FrugalGPT LLM cascade. Our goals are three-fold: (i) understand what a simple instantiation of LLM cascade learns, (ii) quantify the cost savings attained by FrugalGPT while matching the best individual LLM API\u2019s performance, and (iii) measure the trade-offs between performance and cost enabled by FrugalGPT.\n\nSetups: LLM APIs, Tasks, Datasets, and FrugalGPT instances. We have selected 12 LLM APIs from 5 mainstream providers, namely, OpenAI [Ope], AI21 [AI2], CoHere [CoH], Textsynth [Tex], and ForeFrontAI [FFA]. The details are summarized in Table 1. FrugalGPT has been developed on top of these APIs and evaluated on a range of datasets belonging to different tasks, including HEADLINES [SK21], OVERRULING [ZGA + 21], and COQA [RCM19]. The summary of these datasets is presented in Table 2. HEADLINES is a financial news dataset whose goal is to determine the gold price trend (up, down, neutral, or none) by reading financial news titles. This is especially useful for filtering relevant news in financial markets. OVERRULING is a legal document dataset where the goal is to determine whether a given sentence is an overruling, i.e., rejecting previous legal cases. COQA is a reading comprehension dataset developed in a conversational setting, which we have adapted as a direct query answering task. We focus on the LLM cascade approach with a cascade length of 3, as this simplifies the optimization space and already demonstrates good results. Each dataset is randomly split into a training set to learn the LLM cascade and a test set for evaluation.\n\nA Case Study. Let us begin with a case study on the HEADLINES dataset. We set the budget to be $6.5, which is one-fifth of GPT-4\u2019s cost. We employ a DistilBERT [SDCW19] tailored to regression as the scoring function. It is important to note that DistilBERT is considerably smaller and therefore less expensive than all LLMs considered here. As depicted in Figure 3 (a), the learned FrugalGPT sequentially calls GPT-J, J1-L, and GPT-4. For any given query, it first extracts an answer from GPT-J. If the score of this answer is greater than 0.96, the answer is accepted as the final response. Otherwise, J1-L is queried. J1-L\u2019s answer is accepted as the final response if its score is greater than 0.37; otherwise, GPT-4 is invoked to obtain the final answer. Interestingly, this approach outperforms GPT-4 for numerous queries. For instance, given a headline \u201dGold off the lows after dismal U.S.", "start_char_idx": 3204, "end_char_idx": 5890, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62d129b0-50b8-41e2-86ff-56c8e6b669f0": {"__data__": {"id_": "62d129b0-50b8-41e2-86ff-56c8e6b669f0", "embedding": null, "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Strategy 3: LLM cascade"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2c537797-d5cf-418c-be60-8634f22d589c", "node_type": "4", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Strategy 3: LLM cascade"}, "hash": "087040e91b6b31028dac94149cacad752df3dca304f14403ae472f5fb3220af8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c79362ad-30d9-42f6-82d8-a815d0f069ef", "node_type": "1", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Strategy 3: LLM cascade"}, "hash": "49fd7d05b36904d11e2154873e67bb4f5e24f56d762913c155d06ee47dec92cf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23eddab7-dcba-4d6c-bab6-c927bcdd9dc2", "node_type": "1", "metadata": {}, "hash": "42bc0ad99475f6153ffa01c1ac37e1d62373aa3a77e215e4dba1f88b6c6b2a0b", "class_name": "RelatedNodeInfo"}}, "text": "We employ a DistilBERT [SDCW19] tailored to regression as the scoring function. It is important to note that DistilBERT is considerably smaller and therefore less expensive than all LLMs considered here. As depicted in Figure 3 (a), the learned FrugalGPT sequentially calls GPT-J, J1-L, and GPT-4. For any given query, it first extracts an answer from GPT-J. If the score of this answer is greater than 0.96, the answer is accepted as the final response. Otherwise, J1-L is queried. J1-L\u2019s answer is accepted as the final response if its score is greater than 0.37; otherwise, GPT-4 is invoked to obtain the final answer. Interestingly, this approach outperforms GPT-4 for numerous queries. For instance, given a headline \u201dGold off the lows after dismal U.S. GDP data\u201d from NASDAQ, FrugalGPT accurately predicts that the price is going down, while GPT-4\n---\n|Provider|API|Size/B|10M input tokens|10M output tokens|request|\n|---|---|---|---|---|---|\n|GPT-Curie| |6.7|2|2|0|\n|ChatGPT| |NA|2|2|0|\n|OpenAI|GPT-3|175|20|20|0|\n|OpenAI|GPT-4|NA|30|60|0|\n|OpenAI|J1-Large|7.5|0|30|0.0003|\n|AI21|J1-Grande|17|0|80|0.0008|\n|AI21|J1-Jumbo|178|0|250|0.005|\n|Cohere|Xlarge|52|10|10|0|\n|ForeFrontAI|QA|16|5.8|5.8|0|\n|ForeFrontAI|GPT-J|6|0.2|5|0|\n|Textsynth|FAIRSEQ|13|0.6|15|0|\n|Textsynth|GPT-Neox|20|1.4|35|0|\n\n|Dataset|Domain|Size|#Examples in the prompt|\n|---|---|---|---|\n|HEADLINES|Finance|10000|8|\n|OVERRULING|Law|2400|5|\n|COQA|Passage Reading|7982|2|\n|Financial News|GPT-J|score&lt;0.96?|Yes|\n| | | |J1-L|\n| | | |score&lt;0.37?|Yes|\n| | | |GPT-4|No|\n---\n|Content|Page Number|\n|---|---|\n|HEADLINES|10|\n|OVERULLING|10|\n|COQA|10|\n\nFigure 4: Maximum performance improvement (MPI) of each pair of LLMs. (a), (b), and (c) correspond to the three datasets, separately. One entry indicates the percent of cases that the LLM on its row is wrong but the LLM on its column gives the right answer. Overall, we observe that cheap LLMs can be complementary to the expensive ones quite often. For example, for about 6% of the data, GPT-4 makes a mistake but GPJ-J (or J-L or GPT-C) gives the right answer on HEADLINES provides an incorrect answer (as shown in Figure 3(b)). Overall, FrugalGPT results in both accuracy gains and cost reduction. As illustrated in Figure 3(c), its cost is reduced by 80%, while the accuracy is even 1.5% higher.\n\nLLM diversity. Why can multiple LLM APIs potentially produce better performance than the best individual LLM? In essence, this is due to generation diversity: even an inexpensive LLM can sometimes correctly answer queries on which a more expensive LLM fails. To measure this diversity, we use the maximum performance improvement, or MPI. The MPI of LLM A with respect to LLM B is the probability that LLM A generates the correct answer while LLM B provides incorrect ones. This metric essentially measures the maximum performance gains achievable by invoking LLM A in addition to LLM B.\n\nMPI between each pair of LLM APIs for all datasets is displayed in Figure 4. Overall, we observe significant potential within the LLM marketplace.", "start_char_idx": 5132, "end_char_idx": 8188, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "23eddab7-dcba-4d6c-bab6-c927bcdd9dc2": {"__data__": {"id_": "23eddab7-dcba-4d6c-bab6-c927bcdd9dc2", "embedding": null, "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Strategy 3: LLM cascade"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2c537797-d5cf-418c-be60-8634f22d589c", "node_type": "4", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Strategy 3: LLM cascade"}, "hash": "087040e91b6b31028dac94149cacad752df3dca304f14403ae472f5fb3220af8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62d129b0-50b8-41e2-86ff-56c8e6b669f0", "node_type": "1", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Strategy 3: LLM cascade"}, "hash": "5408c67f1f91c0920ef8286eecafffb1fcd609f2aaa9558c8a9061601ac7e67f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e12a9c9e-a954-4f3e-8e7c-2f54aaef5feb", "node_type": "1", "metadata": {}, "hash": "dc5c31b117ccbe17a2bd8c0f6e8805ddda87af63d7d5431f65345955eb0526fc", "class_name": "RelatedNodeInfo"}}, "text": "Overall, FrugalGPT results in both accuracy gains and cost reduction. As illustrated in Figure 3(c), its cost is reduced by 80%, while the accuracy is even 1.5% higher.\n\nLLM diversity. Why can multiple LLM APIs potentially produce better performance than the best individual LLM? In essence, this is due to generation diversity: even an inexpensive LLM can sometimes correctly answer queries on which a more expensive LLM fails. To measure this diversity, we use the maximum performance improvement, or MPI. The MPI of LLM A with respect to LLM B is the probability that LLM A generates the correct answer while LLM B provides incorrect ones. This metric essentially measures the maximum performance gains achievable by invoking LLM A in addition to LLM B.\n\nMPI between each pair of LLM APIs for all datasets is displayed in Figure 4. Overall, we observe significant potential within the LLM marketplace. For instance, GPT-C, GPT-J, and J1-L can all enhance GPT-4\u2019s performance by up to 6% on the HEADLINES dataset. On the COQA dataset, there are 13% of data points where GPT-4 makes an error, but GPT-3 provides the correct answer. Although these improvement upper bounds may not always be attainable, they do demonstrate the possibility of utilizing more affordable services to achieve better performance.\n\n|Dataset|Best invidual LLM|Best individual LLM|FrugalGPT|Cost Savings|\n|---|---|---|---|---|\n|HEADLINES|GPT-4|33.1|0.6|98.3%|\n|OVERULLING|GPT-4|9.7|2.6|73.3%|\n|COQA|GPT-3|72.5|29.6|59.2%|\n\nCost Savings. Subsequently, we examine whether FrugalGPT can reduce costs while maintaining accuracy and, if so, by how much. Table 3 displays the overall cost savings of FrugalGPT, which range from 50% to 98%. This is feasible because FrugalGPT identifies the queries that can be accurately answered by smaller LLMs and, as a result, only invokes those cost-effective LLMs. Powerful but expensive LLMs, such as GPT-4, are utilized only for challenging queries detected by FrugalGPT.\n\nPerformance and Cost Trade-offs. Now, we investigate the trade-offs between performance and cost achieved by FrugalGPT, as illustrated in Figure 5. Several interesting observations can be made.\n---\n|Content|Page Number|\n|---|---|\n|(a) HEADLINES| |\n|FrugalGPT| |\n|The time has come to reconcile and regularize our cases in this field.| |\n|GPT-J|0.91>0.9|\n|Yes| |\n|GPT-4| |\n|No| |\n|(b) OVERRULING| |\n|FrugalGPT| |\n|[..] Cap Winters [...] added a thousand grey hairs to his head [...] Q: Did he have red hair?| |\n|GPT-3| |\n|No 0.8>0.2|No|\n|GPT-4| |\n|The text does not mention this.| |\n|FrugalGPT| |\n|[..] told every Tuesday for their story time.", "start_char_idx": 7284, "end_char_idx": 9909, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e12a9c9e-a954-4f3e-8e7c-2f54aaef5feb": {"__data__": {"id_": "e12a9c9e-a954-4f3e-8e7c-2f54aaef5feb", "embedding": null, "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Strategy 3: LLM cascade"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2c537797-d5cf-418c-be60-8634f22d589c", "node_type": "4", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Strategy 3: LLM cascade"}, "hash": "087040e91b6b31028dac94149cacad752df3dca304f14403ae472f5fb3220af8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23eddab7-dcba-4d6c-bab6-c927bcdd9dc2", "node_type": "1", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Strategy 3: LLM cascade"}, "hash": "ef29c5dc936a805c5f9a89d9b063a10e8fae2ede1e328a4816e6eb36bdb2974f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "42c48a5f-f623-4d93-807d-1bd6108d35e0", "node_type": "1", "metadata": {}, "hash": "02da9449b49ffb2e859871431c9fb181400b44e9eb9f87dd76a28331617fc593", "class_name": "RelatedNodeInfo"}}, "text": "Several interesting observations can be made.\n---\n|Content|Page Number|\n|---|---|\n|(a) HEADLINES| |\n|FrugalGPT| |\n|The time has come to reconcile and regularize our cases in this field.| |\n|GPT-J|0.91>0.9|\n|Yes| |\n|GPT-4| |\n|No| |\n|(b) OVERRULING| |\n|FrugalGPT| |\n|[..] Cap Winters [...] added a thousand grey hairs to his head [...] Q: Did he have red hair?| |\n|GPT-3| |\n|No 0.8>0.2|No|\n|GPT-4| |\n|The text does not mention this.| |\n|FrugalGPT| |\n|[..] told every Tuesday for their story time. [...].Q: when did they have time free?| |\n|GPT-4| |\n|from school| |\n|FrugalGPT| |\n|When I [...] a little black-walnut shelf [...]Q: What was the shelf made of?| |\n|GPT-3|0.1<0.2|\n|J1|0.2<0.3|\n|GPT-4| |\n|black-walnut| |\n\n|Cost ($)|FrugalGPT|\n|---|---|\n|1|X|\n|0.9|Yes|\n|0.8|X|\n|0.7|No|\n|0.6|X|\n|0.5|X|\n|0.4|X|\n|0.35|X|\n|0.3|X|\n|0.25|X|\n|0.2|X|\n\n|Cost ($)|FrugalGPT|\n|---|---|\n|0|X|\n|20|X|\n|40|X|\n|60|X|\n|80|X|\n|100|X|\n|120|X|\n|140|X|\n\nFigure 5: Accuracy and cost tradeoffs achieved by FrugalGPT. Overall, FrugalGPT often achieves the same performance of the best individual LLM API (e.g., GPT-4) with orders of magnitudes smaller cost. When incurring the same cost, FrugalGPT can improve the accuracy by up to 5%. Examples of LLM cascade for each dataset are shown on the right.\n\n9\n---\nFirst, the cost ranking of different LLM APIs is not fixed. For instance, J1 is the second most expensive LLM on the HEADLINES dataset, while GPT-3 holds that position on the OVERRULING and COQA datasets. This is primarily due to the heterogeneous pricing mechanism: J1 incurs a high cost for each generated token but charges nothing for input tokens, whereas GPT-3 charges for both input and output tokens. Moreover, more expensive LLM APIs sometimes result in worse performance than their cheaper counterparts. For example, J1 is costlier than GPT-3 on HEADLINES, but its performance is inferior. These observations underscore the importance of aptly selecting LLM APIs, even in the absence of budget constraints.\n\nNext, we note that FrugalGPT enables smooth performance-cost trade-offs across all evaluated datasets. This offers flexible choices to LLM users and potentially helps LLM API providers save energy and reduce carbon emissions. In fact, FrugalGPT can simultaneously reduce costs and improve accuracy. For example, on the OVERRULING dataset, FrugalGPT achieves a 1% accuracy gain while reducing costs by 73% compared to the best LLM API, GPT-4. This is likely because FrugalGPT integrates knowledge from multiple LLMs.\n\nThe example queries shown in Figure 5 further aid in understanding why FrugalGPT can simultaneously improve performance and reduce costs. GPT-4 makes mistakes on some queries (e.g., the first example in part (a)), but some low-cost APIs provide correct predictions. FrugalGPT accurately identifies those queries and relies solely on the inexpensive APIs. For example, GPT-4 incorrectly infers no overruling from the legal statement \u201dThe time has come to reconcile and regularize our cases in this field,\u201d as shown in Figure 5(b). However, FrugalGPT accepts GPT-J\u2019s correct answer, avoiding the use of expensive LLMs and improving overall performance. Naturally, a single LLM API is not always correct; LLM cascade overcomes this by employing a chain of LLM APIs.", "start_char_idx": 9415, "end_char_idx": 12690, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "42c48a5f-f623-4d93-807d-1bd6108d35e0": {"__data__": {"id_": "42c48a5f-f623-4d93-807d-1bd6108d35e0", "embedding": null, "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Strategy 3: LLM cascade"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2c537797-d5cf-418c-be60-8634f22d589c", "node_type": "4", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Strategy 3: LLM cascade"}, "hash": "087040e91b6b31028dac94149cacad752df3dca304f14403ae472f5fb3220af8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e12a9c9e-a954-4f3e-8e7c-2f54aaef5feb", "node_type": "1", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Strategy 3: LLM cascade"}, "hash": "2bf7710ac8e4ac26bfc97df88a201fec61f0ece8b39a02723b12bbfc623dd91f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2df22843-6401-4731-ac97-76a352c8e919", "node_type": "1", "metadata": {}, "hash": "44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a", "class_name": "RelatedNodeInfo"}}, "text": "This is likely because FrugalGPT integrates knowledge from multiple LLMs.\n\nThe example queries shown in Figure 5 further aid in understanding why FrugalGPT can simultaneously improve performance and reduce costs. GPT-4 makes mistakes on some queries (e.g., the first example in part (a)), but some low-cost APIs provide correct predictions. FrugalGPT accurately identifies those queries and relies solely on the inexpensive APIs. For example, GPT-4 incorrectly infers no overruling from the legal statement \u201dThe time has come to reconcile and regularize our cases in this field,\u201d as shown in Figure 5(b). However, FrugalGPT accepts GPT-J\u2019s correct answer, avoiding the use of expensive LLMs and improving overall performance. Naturally, a single LLM API is not always correct; LLM cascade overcomes this by employing a chain of LLM APIs. For example, in the second example shown in Figure 5(a), FrugalGPT identifies that GPT-J\u2019s generation may not be reliable and turns to the second LLM in the chain, J1-L, to find the correct answer. Again, GPT-4 provides the wrong answer. FrugalGPT is not perfect, and there remains ample room for cost reduction. For example, in the third example in Figure 5(c), all LLM APIs in the chain give the same answer. However, FrugalGPT is unsure if the first LLMs are correct, resulting in the need to query all LLMs in the chain. Identifying how to avoid such cases remains an open problem.", "start_char_idx": 11853, "end_char_idx": 13276, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2df22843-6401-4731-ac97-76a352c8e919": {"__data__": {"id_": "2df22843-6401-4731-ac97-76a352c8e919", "embedding": null, "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": ""}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dd91a59d-c0cd-457b-92e1-daeff24f5e4d", "node_type": "4", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": ""}, "hash": "0136a5b5f9a965ce8e9efa12c4325609a55de202e4643c775dd58ed53128dd32", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "42c48a5f-f623-4d93-807d-1bd6108d35e0", "node_type": "1", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Strategy 3: LLM cascade"}, "hash": "7e38a5a278b4e45823b97c165390e6e0aa0b7a79fefc720fd574b9b4394a8d85", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ad070a2-c493-4d29-86d6-84343d823763", "node_type": "1", "metadata": {}, "hash": "4f28195d87bb6ff426c4a9e8a52c20b9bc428891c93b696fda1e776ff09592b5", "class_name": "RelatedNodeInfo"}}, "text": "", "start_char_idx": 0, "end_char_idx": 0, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1ad070a2-c493-4d29-86d6-84343d823763": {"__data__": {"id_": "1ad070a2-c493-4d29-86d6-84343d823763", "embedding": null, "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "# Discussions, Limitations and Future Prospects"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ea8c3771-a622-4cdc-a507-3418993a81d0", "node_type": "4", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "# Discussions, Limitations and Future Prospects"}, "hash": "0ea646044f50ff1c878fc306af436e0fd59515e99ff749ee36c7b586a6862372", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2df22843-6401-4731-ac97-76a352c8e919", "node_type": "1", "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": ""}, "hash": "0136a5b5f9a965ce8e9efa12c4325609a55de202e4643c775dd58ed53128dd32", "class_name": "RelatedNodeInfo"}}, "text": "# Discussions, Limitations and Future Prospects\n\nThe substantial cost of employing LLMs in real-world scenarios presents a considerable barrier to their widespread usage. In this paper, we outline and discuss practical strategies for reducing the inference cost of using LLM APIs. We also developed FrugalGPT to illustrate one of the cost-saving strategies, LLM cascade. Our empirical findings show that FrugalGPT can reduce costs by up to 98% while preserving the performance of cutting-edge LLMs.\n\nFrugalGPT lays the groundwork for optimizing task performance with LLM APIs under budget constraints; however, it has some limitations. To train the LLM cascade strategy in FrugalGPT, we need some labeled examples. And in order for the cascade to work well, the training examples should be from the same or similar distribution as the test examples. Moreover, learning the LLM cascade itself requires resources. We view this as an one-time upfront cost; this is beneficial when the final query dataset is larger than the data used to train the cascade. There are also other promising strategies for cost saving, such as speeding up attention computation itself, that we do not discuss here. Given the rapid development of LLM, this paper is not meant to be comprehensive or to provide a definitive solution. Our goal is to lay a foundation for this important research agenda and to demonstrate that even simple cascade can already achieve promising savings.\n\nThere are also many related directions for future exploration. While FrugalGPT concentrates on balancing performance and cost, real-world applications call for the evaluation of other critical factors, including latency, fairness, privacy, and environmental impact. Incorporating these elements into optimization methodologies while maintaining performance and cost-effectiveness is an important avenue for future research. Furthermore, utilizing LLMs in risk-critical applications necessitates the careful quantification of uncertainty in LLM-generated outputs. As the field progresses, addressing the environmental ramifications of training and deploying LLMs demands a joint effort from LLM users and API providers. The continuous evolution of LLMs and their applications will inevitably unveil new challenges and opportunities, fostering further research and development in this dynamic field.\n---", "start_char_idx": 0, "end_char_idx": 2360, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"f0c3b0dc-5531-4658-9108-de5dba626622": {"doc_hash": "0136a5b5f9a965ce8e9efa12c4325609a55de202e4643c775dd58ed53128dd32", "ref_doc_id": "f17539b1-7770-464f-865b-61e017f7f8cc"}, "e7d898e8-024f-48b3-8b43-42852104a39f": {"doc_hash": "c3f851ebc1e64a06052801fa8a66f6dea925ffdfbd77f8e8ffa8c0c28ad2a7de", "ref_doc_id": "12e1ac98-936e-42c5-9d74-ed4380e1b47e"}, "05c80f1b-e6a6-4ab6-acf0-ff0c0823b277": {"doc_hash": "0a4247beb51ae14f5fd9fc1add893ba4b08b60f189057f26ba8bc7b4d7f3cef2", "ref_doc_id": "1b41fc57-e6a8-424d-8b30-55512106d4ac"}, "1dbb4a56-82a7-451b-a142-4cb73aaeb41e": {"doc_hash": "35579c8c55e4ed892980e1354d093e06e37c2dd6b645fb66ae237033beb76654", "ref_doc_id": "1c056585-412e-4b72-94f5-579ca13e03ea"}, "606ade99-9106-496c-92ee-2936ed1edb5b": {"doc_hash": "4e7106b08f8a6edff464fcd1249e21f01eb7b3851f5369f197253593c48dfc84", "ref_doc_id": "96877b40-1d5f-4930-afc9-55a97ce603ea"}, "6c20661c-50e8-4944-8083-d55712ff2edc": {"doc_hash": "87abd2882c0bc359a52537f87af193b5bd9ab48a189100186006a50ce45a30af", "ref_doc_id": "8e16fc9f-534d-405f-80a8-38fd1681ccf7"}, "1d07e0fb-15d0-4957-9e3b-6df8e0572f68": {"doc_hash": "fe7c58d70b811d8fe3f40aed2a6bfaeee2567e246db87cc12097daac6c02e713", "ref_doc_id": "c6339632-993f-4bfd-ba19-dadb3e70fd37"}, "79b89db4-c2c3-4c81-a0e1-4f8c02c5a2ee": {"doc_hash": "0638f614fd2d3ad811c7e835b6ac814abc9b9ba228ad9a53210cbccc7dfdc6ca", "ref_doc_id": "921c6ce1-b555-414d-bfa7-51b2bb7da68a"}, "8d79d00a-64d0-44eb-9b3e-054da2854340": {"doc_hash": "1ca6867a49ba7ae052a4694e3dddf7abfe4d2edf9f6f6ddf5a77d1fa904689df", "ref_doc_id": "9f3fd947-0e73-417e-bf3a-e0b168e6edb8"}, "5fd8b287-39ba-48eb-a041-fdcf9494f262": {"doc_hash": "8d76db5da3976419a466af942474ace3173e7f55cf413336a7bc7336bb173549", "ref_doc_id": "0d9b0dea-c6b0-464e-8759-23a455fa7555"}, "b532ead4-cc32-4cdd-8c65-d0d219af4807": {"doc_hash": "7f319c59c703166d27977a5b7e5073ec367c1a9bad70e7428a844a044a7eaf2c", "ref_doc_id": "008d72aa-528f-487c-8b90-59d2c0e05907"}, "26abc728-9d71-42b8-93ec-1697a7f125a8": {"doc_hash": "13531de92aa949f1904a4aeaf114d3b3b6a978105aeb87718aa82a258e0f888b", "ref_doc_id": "a677f3e2-6f11-4bc1-9fb7-57f347a154ed"}, "77cea1c8-7602-4149-9c6f-4f0759104eb8": {"doc_hash": "1c5c1aedab561b722f81f615cf82cfe56b7cf34afa428dcc57346b5434ddd207", "ref_doc_id": "66a13b36-b55f-4903-9662-51f875698037"}, "8a9c809a-2f7e-4c52-a139-3b4267b0a822": {"doc_hash": "46fc4246d0b9fc44f93e63d214019379e7f1440832c1c7448d1b625aa5325ffd", "ref_doc_id": "7868ef7a-460f-45bf-a886-f1eb69966961"}, "71b1f06f-defc-4a8f-9c76-37a8b1788f3a": {"doc_hash": "e2b23d394c6c7672fbde9042a259c4e28e102ef877bae1256cd1258bae541bd2", "ref_doc_id": "4f5d3b28-49ec-4969-aa29-a97dbb2322d9"}, "47ba160e-b309-4e00-b846-4ad36333b462": {"doc_hash": "47a0fdc1a383e17cbe667cd15525736117462b57e6d2d1813b9ad7eef594b84d", "ref_doc_id": "95428b11-3349-4ffc-ae2a-f65c60bad8dc"}, "997d78a4-9eda-4ef5-98be-1de70a453fde": {"doc_hash": "eb439d41c6e0420ae58d6b7312276873936c34f0e9a08fd117a84acc40b71ce7", "ref_doc_id": "2c537797-d5cf-418c-be60-8634f22d589c"}, "c79362ad-30d9-42f6-82d8-a815d0f069ef": {"doc_hash": "49fd7d05b36904d11e2154873e67bb4f5e24f56d762913c155d06ee47dec92cf", "ref_doc_id": "2c537797-d5cf-418c-be60-8634f22d589c"}, "62d129b0-50b8-41e2-86ff-56c8e6b669f0": {"doc_hash": "5408c67f1f91c0920ef8286eecafffb1fcd609f2aaa9558c8a9061601ac7e67f", "ref_doc_id": "2c537797-d5cf-418c-be60-8634f22d589c"}, "23eddab7-dcba-4d6c-bab6-c927bcdd9dc2": {"doc_hash": "ef29c5dc936a805c5f9a89d9b063a10e8fae2ede1e328a4816e6eb36bdb2974f", "ref_doc_id": "2c537797-d5cf-418c-be60-8634f22d589c"}, "e12a9c9e-a954-4f3e-8e7c-2f54aaef5feb": {"doc_hash": "2bf7710ac8e4ac26bfc97df88a201fec61f0ece8b39a02723b12bbfc623dd91f", "ref_doc_id": "2c537797-d5cf-418c-be60-8634f22d589c"}, "42c48a5f-f623-4d93-807d-1bd6108d35e0": {"doc_hash": "7e38a5a278b4e45823b97c165390e6e0aa0b7a79fefc720fd574b9b4394a8d85", "ref_doc_id": "2c537797-d5cf-418c-be60-8634f22d589c"}, "2df22843-6401-4731-ac97-76a352c8e919": {"doc_hash": "0136a5b5f9a965ce8e9efa12c4325609a55de202e4643c775dd58ed53128dd32", "ref_doc_id": "dd91a59d-c0cd-457b-92e1-daeff24f5e4d"}, "1ad070a2-c493-4d29-86d6-84343d823763": {"doc_hash": "8e38fe359eb33c5525b5165ed787f8cd09cbad8922dbe5ce97d1fe45cffd88ef", "ref_doc_id": "ea8c3771-a622-4cdc-a507-3418993a81d0"}}, "docstore/ref_doc_info": {"f17539b1-7770-464f-865b-61e017f7f8cc": {"node_ids": ["f0c3b0dc-5531-4658-9108-de5dba626622"], "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": ""}}, "12e1ac98-936e-42c5-9d74-ed4380e1b47e": {"node_ids": ["e7d898e8-024f-48b3-8b43-42852104a39f"], "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance"}}, "1b41fc57-e6a8-424d-8b30-55512106d4ac": {"node_ids": ["05c80f1b-e6a6-4ab6-acf0-ff0c0823b277"], "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "# Abstract"}}, "1c056585-412e-4b72-94f5-579ca13e03ea": {"node_ids": ["1dbb4a56-82a7-451b-a142-4cb73aaeb41e"], "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "# Introduction"}}, "96877b40-1d5f-4930-afc9-55a97ce603ea": {"node_ids": ["606ade99-9106-496c-92ee-2936ed1edb5b"], "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Query"}}, "8e16fc9f-534d-405f-80a8-38fd1681ccf7": {"node_ids": ["6c20661c-50e8-4944-8083-d55712ff2edc"], "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Query"}}, "c6339632-993f-4bfd-ba19-dadb3e70fd37": {"node_ids": ["1d07e0fb-15d0-4957-9e3b-6df8e0572f68"], "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Budget"}}, "921c6ce1-b555-414d-bfa7-51b2bb7da68a": {"node_ids": ["79b89db4-c2c3-4c81-a0e1-4f8c02c5a2ee"], "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Figure 1: Our vision for reducing LLM cost while improving accuracy."}}, "9f3fd947-0e73-417e-bf3a-e0b168e6edb8": {"node_ids": ["8d79d00a-64d0-44eb-9b3e-054da2854340"], "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "1, we discuss three main strategies for cost reduction:"}}, "0d9b0dea-c6b0-464e-8759-23a455fa7555": {"node_ids": ["5fd8b287-39ba-48eb-a041-fdcf9494f262"], "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Related Works"}}, "008d72aa-528f-487c-8b90-59d2c0e05907": {"node_ids": ["b532ead4-cc32-4cdd-8c65-d0d219af4807"], "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "LLM APIs and ML-as-a-Service"}}, "a677f3e2-6f11-4bc1-9fb7-57f347a154ed": {"node_ids": ["26abc728-9d71-42b8-93ec-1697a7f125a8"], "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Organization of the Paper"}}, "66a13b36-b55f-4903-9662-51f875698037": {"node_ids": ["77cea1c8-7602-4149-9c6f-4f0759104eb8"], "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Scope and Problem Statement"}}, "7868ef7a-460f-45bf-a886-f1eb69966961": {"node_ids": ["8a9c809a-2f7e-4c52-a139-3b4267b0a822"], "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "How to Use LLMs Affordably and Accurately"}}, "4f5d3b28-49ec-4969-aa29-a97dbb2322d9": {"node_ids": ["71b1f06f-defc-4a8f-9c76-37a8b1788f3a"], "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Strategy 1: Prompt adaptation"}}, "95428b11-3349-4ffc-ae2a-f65c60bad8dc": {"node_ids": ["47ba160e-b309-4e00-b846-4ad36333b462"], "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Strategy 2: LLM approximation"}}, "2c537797-d5cf-418c-be60-8634f22d589c": {"node_ids": ["997d78a4-9eda-4ef5-98be-1de70a453fde", "c79362ad-30d9-42f6-82d8-a815d0f069ef", "62d129b0-50b8-41e2-86ff-56c8e6b669f0", "23eddab7-dcba-4d6c-bab6-c927bcdd9dc2", "e12a9c9e-a954-4f3e-8e7c-2f54aaef5feb", "42c48a5f-f623-4d93-807d-1bd6108d35e0"], "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "Strategy 3: LLM cascade"}}, "dd91a59d-c0cd-457b-92e1-daeff24f5e4d": {"node_ids": ["2df22843-6401-4731-ac97-76a352c8e919"], "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": ""}}, "ea8c3771-a622-4cdc-a507-3418993a81d0": {"node_ids": ["1ad070a2-c493-4d29-86d6-84343d823763"], "metadata": {"file_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "paper_name": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "section": "# Discussions, Limitations and Future Prospects"}}}}